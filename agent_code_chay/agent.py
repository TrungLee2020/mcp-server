import json
import logging
import time
import traceback
from typing import Any

import uvicorn
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.apps import A2AStarletteApplication
from a2a.server.events import EventQueue
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import AgentCapabilities, AgentCard, AgentSkill
from a2a.utils import new_agent_text_message
from openai import OpenAI

from agent_code_chay.utils import process_stream

from .a2a_client import CustomA2AClient
from .mcp_client import SSEMCPClient, STDIOMCPClient, StreamableHTTPMCPClient

llm_logger = logging.getLogger("LLM")


class Agent:
    """Class ƒë·∫°i di·ªán cho m·ªôt agent t√≠ch h·ª£p v·ªõi LLM (t∆∞∆°ng t√°c d∆∞·ªõi d·∫°ng OpenAI client) c√≥ kh·∫£ nƒÉng g·ªçi tool, giao ti·∫øp v·ªõi MCP server v√† ho·∫°t ƒë·ªông nh∆∞ remote agent.

    Class n√†y c√≥ th·ªÉ stream d·ªØ li·ªáu t·ª´ LLM, x·ª≠ l√Ω tool calls, ph·ª•c v·ª• tr√™n giao di·ªán ng∆∞·ªùi d√πng (Streamlit) ho·∫∑c nh∆∞ m·ªôt remote agent.

    Attributes:
        openai_client (OpenAI): Client t∆∞∆°ng th√≠ch v·ªõi th∆∞ vi·ªán OpenAI ƒë·ªÉ g·ªçi API LLM.
        model_name (str): T√™n m√¥ h√¨nh LLM ƒë∆∞·ª£c s·ª≠ d·ª•ng.
        temperature (float): Temperature khi g·ªçi LLM, ·∫£nh h∆∞·ªüng ƒë·∫øn ƒë·ªô s√°ng t·∫°o c·ªßa ƒë·∫ßu ra.
        system_message (str): System message ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng c√°ch tr·∫£ l·ªùi c·ªßa LLM.
        stream (bool): Ph·∫£n h·ªìi c·ªßa LLM c√≥ ·ªü d·∫°ng stream hay kh√¥ng
        tools (dict): Danh s√°ch c√°c tool m√† agent c√≥ th·ªÉ g·ªçi.
        stdio_mcp_server_commands (list[list[Any]]): Danh s√°ch l·ªánh ƒë·ªÉ kh·ªüi t·∫°o stdio MCP servers. C√°c tool t·ª´ mcp server s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t v√†o danh s√°ch tool c·ªßa agent.
        sse_mcp_server_urls (list[str]): Danh s√°ch URL SSE MCP servers. C√°c tool t·ª´ mcp server s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t v√†o danh s√°ch tool c·ªßa agent.
        streamable_http_mcp_server_urls (list[str]): Danh s√°ch URL streamable HTTP MCP servers. C√°c tool t·ª´ mcp server s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t v√†o danh s√°ch tool c·ªßa agent.
        remote_agent_urls (list[str]): Danh s√°ch URL remote agents. C√°c remote agent s·∫Ω ƒë∆∞·ª£c cung c·∫•p cho agent d∆∞·ªõi d·∫°ng tool.
    """

    def __init__(
        self,
        openai_client: OpenAI,
        model_name: str,
        temperature: float,
        system_message: str,
        stream: bool=False,
        tools: dict = [],
        stdio_mcp_server_commands: list[list[Any]] = [],
        sse_mcp_server_urls: list[str] = [],
        streamable_http_mcp_server_urls: list[str] = [],
        remote_agent_urls: list[str] = [],
    ):
        self.openai_client = openai_client
        self.model_name = model_name
        self.temperature = temperature
        self.system_message = system_message
        self.stream = stream
        # self.tools = tools # TODO: Cho ph√©p cung c·∫•p tool tr·ª±c ti·∫øp khi kh·ªüi t·∫°o Agent
        self.stdio_mcp_server_commands = stdio_mcp_server_commands
        self.sse_mcp_server_urls = sse_mcp_server_urls
        self.streamable_http_mcp_server_urls = streamable_http_mcp_server_urls
        self.remote_agent_urls = remote_agent_urls

    @classmethod
    async def create(cls, *args, **kwargs) -> "Agent":
        """Kh·ªüi t·∫°o agent c√πng v·ªõi MCP servers v√† remote agents.

        S·ª≠ d·ª•ng method n√†y ƒë·ªÉ t·∫°o instance thay v√¨ __init__.
        L√Ω do: trong python h√†m __init__ kh√¥ng ƒë∆∞·ª£c ph√©p l√† h√†m async -> D√πng async classmethod create ƒë·ªÉ kh·ªüi t·∫°o.

        Returns:
            Agent: M·ªôt instance c·ªßa Agent ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi tools v√† MCP servers.

        Raises:
            KeyError: N·∫øu c√≥ tool tr√πng l·∫∑p gi·ªØa c√°c MCP servers ho·∫∑c remote agents.

        Example:
            ```python
            client = OpenAI(
                base_url=os.getenv("LLM_BASE_URL"),
                api_key=os.getenv("LLM_API_KEY"),
            )

            agent = await Agent.create(
                openai_client=client,
                model_name=os.getenv("MODEL_NAME"),
                temperature=0,
                system_message="B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªØu √≠ch.",
                tools=[],
                stdio_mcp_server_commands=[["python", ["-m", "mcp_server"]]],
                # sse_mcp_server_urls=["http://localhost:8001/sse"],
                # streamable_http_mcp_server_urls=["http://localhost:8001/mcp"],
                # remote_agent_urls=[],
            )
            ```
        """
        self = cls(*args, **kwargs)
        self.tools = {}
        self.mcp_servers = []

        # Th√™m stdio MCP servers
        for stdio_mcp_server_command in self.stdio_mcp_server_commands:
            stdio_mcp_client = None
            try:
                stdio_mcp_client = await STDIOMCPClient.create(
                    *stdio_mcp_server_command
                )
                conflict = set(self.tools) & set(stdio_mcp_client.tools)
                if conflict:
                    raise KeyError(f"‚ùå Tool tr√πng l·∫∑p: {', '.join(conflict)}")

                self.mcp_servers.append(stdio_mcp_client)
                self.tools.update(stdio_mcp_client.tools)
                logging.info(
                    f"‚úÖ ƒê√£ th√™m stdio MCP server: {stdio_mcp_server_command} v·ªõi tools: {list(stdio_mcp_client.tools.keys())}"
                )
            except:
                logging.info(
                    f"‚ùå Th√™m stdio MCP server th·∫•t b·∫°i: {stdio_mcp_server_command}\n{traceback.format_exc()}"
                )
                if stdio_mcp_client:
                    await stdio_mcp_client.close()

        # Th√™m SSE MCP servers
        for sse_mcp_server_url in self.sse_mcp_server_urls:
            sse_mcp_client = None
            try:
                sse_mcp_client = await SSEMCPClient.create(sse_mcp_server_url)
                conflict = set(self.tools) & set(sse_mcp_client.tools)
                if conflict:
                    raise KeyError(f"‚ùå Tool tr√πng l·∫∑p: {', '.join(conflict)}")

                self.mcp_servers.append(sse_mcp_client)
                self.tools.update(sse_mcp_client.tools)
                logging.info(
                    f"‚úÖ ƒê√£ th√™m SSE MCP server: {sse_mcp_server_url} v·ªõi tools: {list(sse_mcp_client.tools.keys())}"
                )
            except:
                logging.info(
                    f"‚ùå Th√™m SSE MCP server th·∫•t b·∫°i: {sse_mcp_server_url}\n{traceback.format_exc()}"
                )
                if sse_mcp_client:
                    await sse_mcp_client.close()

        # Th√™m streamable HTTP MCP servers
        for streamable_http_mcp_server_url in self.streamable_http_mcp_server_urls:
            streamable_http_mcp_client = None
            try:
                streamable_http_mcp_client = await StreamableHTTPMCPClient.create(
                    streamable_http_mcp_server_url
                )
                conflict = set(self.tools) & set(streamable_http_mcp_client.tools)
                if conflict:
                    raise KeyError(f"‚ùå Tool tr√πng l·∫∑p: {', '.join(conflict)}")

                self.mcp_servers.append(streamable_http_mcp_client)
                self.tools.update(streamable_http_mcp_client.tools)
                logging.info(
                    f"‚úÖ ƒê√£ th√™m Streamable HTTP MCP server: {streamable_http_mcp_server_url} v·ªõi tools: {list(streamable_http_mcp_client.tools.keys())}"
                )
            except:
                logging.info(
                    f"‚ùå Th√™m Streamable HTTP MCP server th·∫•t b·∫°i: {streamable_http_mcp_server_url}\n{traceback.format_exc()}"
                )
                if streamable_http_mcp_client:
                    await streamable_http_mcp_client.close()

        # Th√™m remote agents
        if self.remote_agent_urls:
            try:
                a2a_client = CustomA2AClient(a2a_server_urls=self.remote_agent_urls)
                conflict = set(self.tools) & set(a2a_client.tools)
                if conflict:
                    raise KeyError(f"‚ùå Tool tr√πng l·∫∑p: {', '.join(conflict)}")
                self.tools.update(a2a_client.tools)
            except:
                logging.info(
                    f"‚ùå Th√™m remote agent th·∫•t b·∫°i: {self.remote_agent_urls}\n{traceback.format_exc()}"
                )

        return self

    async def invoke(self, messages: list[dict]) -> list[dict]:
        """G·ªçi LLM v·ªõi danh s√°ch messages v√† x·ª≠ l√Ω tool calls n·∫øu c√≥.

        Args:
            messages (list[dict]): Danh s√°ch messages ƒë·∫ßu v√†o.

        Returns:
            list[dict]: Danh s√°ch messages sau khi LLM ph·∫£n h·ªìi v√† x·ª≠ l√Ω tool calls.
        """
        # Th√™m system message
        if self.system_message:
            messages.insert(0, {"role": "system", "content": self.system_message})

        if self.stream:
            # G·ªçi ƒë·∫øn LLM (stream)
            t0 = time.perf_counter()
            stream = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                tools=[t.tool_define for t in self.tools.values()],
                stream=True,
                temperature=self.temperature,
            )

            # X·ª≠ l√Ω stream
            message, first_response_time, end_response_time = process_stream(
                stream, print_output=False, streamlit_display=False
            )
        else:
            # G·ªçi ƒë·∫øn LLM (non-stream)
            t0 = time.perf_counter()
            response = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                tools=[t.tool_define for t in self.tools.values()],
                stream=False,
                temperature=self.temperature,
            )
            first_response_time = end_response_time = time.perf_counter()
            message = {
                "role": "assistant",
                "content": response.choices[0].message.content.strip(),
                "tool_calls": response.choices[0].message.tool_calls,
            }

        messages.append(message)

        # Log LLM Call
        llm_logger.debug(
            f"\n\tB·∫Øt ƒë·∫ßu ph·∫£n h·ªìi: {first_response_time-t0:.4f}s. "
            f"\n\tT·ªïng th·ªùi gian: {end_response_time-t0:.4f}s."
            f"\n\tTools: {[t.tool_define for t in self.tools.values()]}"
            f"\n\tMessages: {messages}"
        )

        # L·∫∑p l·∫°i cho ƒë·∫øn khi kh√¥ng c√≤n tool calls
        tool_calls = message["tool_calls"]
        while tool_calls:
            for tool_call in tool_calls:
                # G·ªçi tool
                name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                result = await self.tools[name](args)

                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": str(result),
                    }
                )

            # G·ªçi l·∫°i LLM v·ªõi k·∫øt qu·∫£ tool
            if self.stream:
                # G·ªçi ƒë·∫øn LLM (stream)
                t0 = time.perf_counter()
                stream = self.openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    tools=[t.tool_define for t in self.tools.values()],
                    stream=True,
                    temperature=self.temperature,
                )
                message, first_response_time, end_response_time = process_stream(
                    stream, print_output=False, streamlit_display=False
                )
            else:
                # G·ªçi ƒë·∫øn LLM (non-stream)
                t0 = time.perf_counter()
                response = self.openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    tools=[t.tool_define for t in self.tools.values()],
                    stream=False,
                    temperature=self.temperature,
                )
                first_response_time = end_response_time = time.perf_counter()
                message = {
                    "role": "assistant",
                    "content": response.choices[0].message.content.strip(),
                    "tool_calls": response.choices[0].message.tool_calls,
                }

            messages.append(message)
            llm_logger.debug(
                f"\n\tB·∫Øt ƒë·∫ßu ph·∫£n h·ªìi: {first_response_time-t0:.4f}s."
                f"\n\tT·ªïng th·ªùi gian: {end_response_time-t0:.4f}s."
                f"\n\tTools: {[t.tool_define for t in self.tools.values()]}"
                f"\n\tMessages: {messages}"
            )
            tool_calls = message["tool_calls"]

        # X√≥a system message tr∆∞·ªõc khi tr·∫£ v·ªÅ
        if messages[0]["role"] == "system":
            messages.pop(0)
        return messages

    async def serve_as_chat_ui(self):
        """Kh·ªüi ch·∫°y giao di·ªán chatbot v·ªõi Streamlit.

        Cho ph√©p ng∆∞·ªùi d√πng chat, hi·ªÉn th·ªã tool calls v√† k·∫øt qu·∫£ th·ª±c thi ngay trong UI.

        Example:
            ```python
            await agent.serve_as_chat_ui()
            ```
        """
        import streamlit as st

        st.title("Chatbot")

        if "messages" not in st.session_state:
            st.session_state.messages = [
                {"role": "system", "content": self.system_message}
            ]

        # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat (user + assistant)
        messages = [
            m for m in st.session_state.messages if m["role"] in ["user", "assistant"]
        ]
        i = 0
        while i < len(messages):
            if messages[i]["role"] == "user":
                with st.chat_message("user"):
                    st.markdown(messages[i]["content"])
                i += 1
                continue

            if messages[i]["role"] == "assistant":
                with st.chat_message("assistant"):
                    while i < len(messages) and messages[i]["role"] != "user":
                        response_placeholder = st.empty()
                        if messages[i]["content"].strip():
                            response_placeholder.markdown(
                                messages[i]["content"].strip()
                            )
                        # Hi·ªÉn th·ªã tool calls
                        tool_calls = messages[i].get("tool_calls")
                        if tool_calls:
                            for tool_call in tool_calls:
                                tool_id = tool_call.id
                                with st.popover(f"üîß {tool_call.function.name}"):
                                    st.code(
                                        f"Arguments: {tool_call.function.arguments}"
                                    )
                                    # T√¨m k·∫øt qu·∫£ tool call theo id
                                    for m in st.session_state.messages:
                                        if (
                                            m["role"] == "tool"
                                            and m["tool_call_id"] == tool_id
                                        ):
                                            st.code(f"Result: {m['content']}")
                                            break
                        i += 1

        # X·ª≠ l√Ω input t·ª´ ng∆∞·ªùi d√πng
        if prompt := st.chat_input("B·∫°n mu·ªën h·ªèi g√¨?"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                if self.stream:
                    t0 = time.perf_counter()
                    stream = self.openai_client.chat.completions.create(
                        model=self.model_name,
                        messages=st.session_state.messages,
                        tools=[t.tool_define for t in self.tools.values()],
                        stream=True,
                        temperature=self.temperature,
                    )
                    message, first_response_time, end_response_time = process_stream(
                        stream, print_output=False, streamlit_display=True
                    )
                else:
                    t0 = time.perf_counter()
                    response = self.openai_client.chat.completions.create(
                        model=self.model_name,
                        messages=st.session_state.messages,
                        tools=[t.tool_define for t in self.tools.values()],
                        stream=False,
                        temperature=self.temperature,
                    )
                    first_response_time = end_response_time = time.perf_counter()
                    message = {
                        "role": "assistant",
                        "content": response.choices[0].message.content.strip(),
                        "tool_calls": response.choices[0].message.tool_calls,
                    }
                    response_placeholder = st.empty()
                    if message["content"].strip():
                        response_placeholder.markdown(message["content"].strip())

                st.session_state.messages.append(message)

                llm_logger.debug(
                    f"\n\tB·∫Øt ƒë·∫ßu ph·∫£n h·ªìi: {first_response_time-t0:.4f}s. "
                    f"\n\tT·ªïng th·ªùi gian: {end_response_time-t0:.4f}s."
                    f"\n\tTools: {[t.tool_define for t in self.tools.values()]}"
                    f"\n\tMessages: {st.session_state.messages}"
                )

                # X·ª≠ l√Ω tool calls trong Streamlit
                tool_calls = message["tool_calls"]
                while tool_calls:
                    for tool_call in tool_calls:
                        name = tool_call.function.name
                        args = json.loads(tool_call.function.arguments)

                        with st.popover(f"üîß {tool_call.function.name}"):
                            st.code(f"Arguments: {tool_call.function.arguments}")
                            result = await self.tools[name](args)
                            st.code(f"Result: {result}")

                        st.session_state.messages.append(
                            {
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": str(result),
                            }
                        )

                    # G·ªçi l·∫°i LLM v·ªõi k·∫øt qu·∫£ tool
                    if self.stream:
                        t0 = time.perf_counter()
                        stream = self.openai_client.chat.completions.create(
                            model=self.model_name,
                            messages=st.session_state.messages,
                            tools=[t.tool_define for t in self.tools.values()],
                            stream=True,
                            temperature=self.temperature,
                        )
                        message, first_response_time, end_response_time = process_stream(
                            stream, print_output=False, streamlit_display=True
                        )
                    else:
                        t0 = time.perf_counter()
                        response = self.openai_client.chat.completions.create(
                            model=self.model_name,
                            messages=st.session_state.messages,
                            tools=[t.tool_define for t in self.tools.values()],
                            stream=False,
                            temperature=self.temperature,
                        )
                        first_response_time = end_response_time = time.perf_counter()
                        message = {
                            "role": "assistant",
                            "content": response.choices[0].message.content.strip(),
                            "tool_calls": response.choices[0].message.tool_calls,
                        }
                        response_placeholder = st.empty()
                        if message["content"].strip():
                            response_placeholder.markdown(message["content"].strip())
                    st.session_state.messages.append(message)

                    llm_logger.debug(
                        f"\n\tB·∫Øt ƒë·∫ßu ph·∫£n h·ªìi: {first_response_time-t0:.4f}s. "
                        f"\n\tT·ªïng th·ªùi gian: {end_response_time-t0:.4f}s."
                        f"\n\tTools: {[t.tool_define for t in self.tools.values()]}"
                        f"\n\tMessages: {st.session_state.messages}"
                    )
                    tool_calls = message["tool_calls"]

        await self.close()

    async def close(self):
        """ƒê√≥ng t·∫•t c·∫£ k·∫øt n·ªëi"""
        for mcp_server in self.mcp_servers:
            await mcp_server.close()

    async def serve_as_remote_agent(self, agent_card: AgentCard, port: int = 11001):
        """Kh·ªüi ch·∫°y agent nh∆∞ m·ªôt remote agent v·ªõi A2A protocol.

        Args:
            agent_card (AgentCard): Agent card c·ªßa agent.
            port (int, optional): C·ªïng ch·∫°y server. M·∫∑c ƒë·ªãnh l√† 11001.
        """

        class CustomAgentExecutor(AgentExecutor):
            """Custom executor d√πng ƒë·ªÉ x·ª≠ l√Ω request t·ª´ remote agent."""

            def __init__(self, agent: "Agent"):
                self.agent = agent

            async def execute(
                self,
                context: RequestContext,
                event_queue: EventQueue,
            ) -> None:
                """X·ª≠ l√Ω y√™u c·∫ßu t·ª´ user v√† ph·∫£n h·ªìi l·∫°i qua event queue."""
                messages = await self.agent.invoke(
                    [{"role": "user", "content": context.get_user_input()}]
                ) # System message s·∫Ω ƒë∆∞·ª£c th√™m trong method invoke
                final_message = messages[-1]["content"]
                await event_queue.enqueue_event(new_agent_text_message(final_message))

            async def cancel(
                self, context: RequestContext, event_queue: EventQueue
            ) -> None:
                """Hi·ªán t·∫°i ch∆∞a h·ªó tr·ª£ cancel request."""
                raise Exception("cancel not supported")

        request_handler = DefaultRequestHandler(
            agent_executor=CustomAgentExecutor(self),
            task_store=InMemoryTaskStore(),
        )

        server = A2AStarletteApplication(
            agent_card=agent_card,
            http_handler=request_handler,
        )

        # Ch·∫°y uvicorn trong async context
        config = uvicorn.Config(server.build(), host="0.0.0.0", port=port)
        server_instance = uvicorn.Server(config)
        await server_instance.serve()
