# ğŸ“‚ agent_code_chay

ThÆ° má»¥c `agent_code_chay` chá»©a mÃ£ nguá»“n Python phá»¥c vá»¥ cho viá»‡c xÃ¢y dá»±ng vÃ  cháº¡y má»™t **AI Agent** cÃ³ kháº£ nÄƒng:
- Káº¿t ná»‘i tá»›i nhiá»u loáº¡i **MCP (Model Context Protocol) server** khÃ¡c nhau (stdio, SSE, streamable HTTP).
- Quáº£n lÃ½ vÃ  gá»i **tool** do MCP server cung cáº¥p.
- Xá»­ lÃ½ pháº£n há»“i **streaming** tá»« LLM (Large Language Model), bao gá»“m cáº£ text vÃ  tool calls.
- Cung cáº¥p cÃ¡c hÃ m tiá»‡n Ã­ch há»— trá»£ hiá»ƒn thá»‹ vÃ  xá»­ lÃ½ dá»¯ liá»‡u.

Cáº¥u trÃºc thÆ° má»¥c:

```

agent_code_chay/
â”‚â”€â”€ a2a_client.py       # Client káº¿t ná»‘i Agent-to-Agent
â”‚â”€â”€ agent.py            # Äá»‹nh nghÄ©a Agent, Ä‘iá»u phá»‘i tool vÃ  luá»“ng há»™i thoáº¡i
â”‚â”€â”€ mcp_client.py       # Äá»‹nh nghÄ©a cÃ¡c client káº¿t ná»‘i tá»›i MCP server
â”‚â”€â”€ tool.py             # Äá»‹nh nghÄ©a lá»›p Tool vÃ  logic gá»i tool
â”‚â”€â”€ utils.py            # Xá»­ lÃ½ stream tá»« LLM, tiá»‡n Ã­ch há»— trá»£
â””â”€â”€ __init__.py         # ÄÃ¡nh dáº¥u Ä‘Ã¢y lÃ  Python package

````

---

## ğŸ“Œ Ná»™i dung chi tiáº¿t cÃ¡c file

### 1. `a2a_client.py`
- Quáº£n lÃ½ **káº¿t ná»‘i Agent-to-Agent** (A2A).
- Cho phÃ©p má»™t agent cÃ³ thá»ƒ giao tiáº¿p hoáº·c gá»i tool tá»« agent khÃ¡c thÃ´ng qua chuáº©n MCP.
- ÄÃ³ng vai trÃ² káº¿t ná»‘i giá»¯a cÃ¡c agent khi triá»ƒn khai **multi-agent system**.

### 2. `agent.py`
- Chá»©a Ä‘á»‹nh nghÄ©a lá»›p `Agent`, chá»‹u trÃ¡ch nhiá»‡m:
  - Káº¿t ná»‘i tá»›i MCP server thÃ´ng qua cÃ¡c client trong `mcp_client.py`.
  - Láº¥y danh sÃ¡ch tool, gá»i tool, vÃ  duy trÃ¬ session.
  - Xá»­ lÃ½ logic Ä‘iá»u phá»‘i giá»¯a LLM, mcp server vÃ  ngÆ°á»i dÃ¹ng .
- ÄÃ¢y lÃ  file chÃ­nh cá»§a package, Ä‘Ã³ng vai trÃ² Ä‘iá»u phá»‘i toÃ n bá»™ workflow.

### 3. `mcp_client.py`
- Äá»‹nh nghÄ©a cÃ¡c class client Ä‘á»ƒ káº¿t ná»‘i tá»›i MCP server:
  - `STDIOMCPClient`: káº¿t ná»‘i qua **stdio**.
  - `SSEMCPClient`: káº¿t ná»‘i qua **SSE (Server-Sent Events)**.
  - `StreamableHTTPMCPClient`: káº¿t ná»‘i qua **Streamable HTTP**.

### 4. `tool.py`
- Äá»‹nh nghÄ©a class `Tool`
- Äá»‘i tÆ°á»£ng tá»« class nÃ y sáº½ bao gá»“m tÃªn tool, Ä‘á»‹nh nghÄ©a tool (Ä‘á»ƒ cung cáº¥p cho LLM) vÃ  cÃ¡ch Ä‘á»ƒ thá»±c thi tool

### 5. `utils.py`
- Chá»©a hÃ m **`process_stream()`**:
  - Xá»­ lÃ½ output stream tá»« LLM (bao gá»“m text + tool calls).
  - Hiá»ƒn thá»‹ text dáº§n dáº§n trÃªn **Streamlit UI**.
  - TÃ¡ch riÃªng tool calls vÃ  gom chÃºng láº¡i báº±ng `StreamToolAggregator`.
- Cung cáº¥p **thá»i gian pháº£n há»“i Ä‘áº§u tiÃªn vÃ  cuá»‘i cÃ¹ng** â†’ há»¯u Ã­ch Ä‘á»ƒ benchmark tá»‘c Ä‘á»™ LLM.

### 6. `__init__.py`
- ÄÃ¡nh dáº¥u thÆ° má»¥c lÃ  Python package.

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Táº¡o mÃ´i trÆ°á»ng

- MÃ´i trÆ°á»ng Ä‘Æ°á»£c táº¡o báº±ng uv táº¡i thÆ° má»¥c gá»‘c cá»§a project
```bash
uv sync
```

### 2. VÃ­ dá»¥

```python
import asyncio
import os

from a2a.types import AgentCapabilities, AgentCard, AgentSkill
from dotenv import load_dotenv
from openai import OpenAI

from agent_code_chay.agent import Agent
from log_config import setup_logging

load_dotenv()
setup_logging()


async def main():
    client = OpenAI(
        base_url=os.getenv("LLM_BASE_URL"),
        api_key=os.getenv("LLM_API_KEY"),
    )

    agent = await Agent.create(
        openai_client=client,
        model_name=os.getenv("MODEL_NAME"),
        temperature=0,
        system_message="Báº¡n lÃ  trá»£ lÃ½ há»¯u dá»¥ng",
        # stdio_mcp_server_commands=[["python", ["-m", "mcp_server"]]],
        # sse_mcp_server_urls=["http://localhost:8001/sse"],
        # streamable_http_mcp_server_urls=["http://localhost:8001/mcp"],
        # remote_agent_urls=[],
    )

    # # Bá» comment Ä‘á»ƒ táº¡o giao diá»‡n chat streamlit
    # await agent.serve_as_chat_ui()

    # # Bá» comment Ä‘á»ƒ táº¡o server remote agent
    # agent_card = AgentCard(
    #     name="Hello Good bye agent",
    #     description="Hello Good bye agent",
    #     url=f"http://localhost:11001",
    #     version="1.0.0",
    #     defaultInputModes=["text"],
    #     defaultOutputModes=["text"],
    #     capabilities=AgentCapabilities(streaming=False),
    #     skills=[
    #         AgentSkill(
    #             id="tao_loi_chao_va_tam_biet",
    #             name="Táº¡o lá»i chÃ o vÃ  táº¡m biá»‡t",
    #             description="ÄÆ°a ra lá»i chÃ o hoáº·c lá»i táº¡m biá»‡t khi Ä‘Æ°á»£c cung cáº¥p tÃªn ngÆ°á»i cá»¥ thá»ƒ.",
    #             tags=[],
    #             examples=[
    #                 "Táº¡o lá»i chÃ o cho John",
    #                 "Gá»­i lá»i chÃ o tá»›i Tom",
    #                 "NÃ³i táº¡m biá»‡t vá»›i Scott",
    #             ],
    #         )
    #     ],
    # )
    # await agent.serve_as_remote_agent(
    #     agent_card=agent_card,
    #     port=11001,
    # )

if __name__ == "__main__":
    asyncio.run(main())
```

### LÆ°u Ã½
- Class Agent cho phÃ©p khá»Ÿi táº¡o vá»›i stream=True, tá»©c lÃ  cho phÃ©p pháº£n há»“i tá»« phÃ­a LLM cÃ³ dáº¡ng stream. Tuy nhiÃªn hiá»‡n táº¡i Ä‘ang cÃ³ lá»—i tool tráº£ vá» bá»‹ parse sai khi sá»­ dá»¥ng stream=True vÃ  káº¿t quáº£ tráº£ vá» chá»©a tool call. Recommend: Sá»­ dá»¥ng stream=False (giÃ¡ trá»‹ False lÃ  máº·c Ä‘á»‹nh) khi khá»Ÿi táº¡o agent.
https://github.com/vllm-project/vllm/issues/17614